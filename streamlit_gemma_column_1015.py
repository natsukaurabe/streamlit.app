import streamlit as st
import pandas as pd
import io
from datetime import datetime
import isodate
import re
from googleapiclient.discovery import build
import ollama
import subprocess
import time
import requests
import os
import glob
import json

# --- Ollamaã®ç¢ºèª ---
def check_ollama_status():
    """Ollamaã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹"""
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=2)
        return response.status_code == 200
    except:
        return False

def start_ollama_service():
    """Ollamaã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•ã™ã‚‹ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰"""
    try:
        # Windowsã®å ´åˆ
        if os.name == 'nt':
            subprocess.Popen(['ollama', 'serve'], 
                           creationflags=subprocess.CREATE_NEW_CONSOLE)
        # Mac/Linuxã®å ´åˆ
        else:
            subprocess.Popen(['ollama', 'serve'], 
                           stdout=subprocess.DEVNULL, 
                           stderr=subprocess.DEVNULL)
        time.sleep(3)  # ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•å¾…ã¡
        return True
    except Exception as e:
        st.error(f"Ollamaèµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def ensure_ollama_running():
    """OllamaãŒå®Ÿè¡Œä¸­ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦èµ·å‹•"""
    if not check_ollama_status():
        st.info("Ollamaã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•ã—ã¦ã„ã¾ã™...")
        if start_ollama_service():
            # æœ€å¤§10ç§’å¾…æ©Ÿ
            for i in range(10):
                if check_ollama_status():
                    st.success("Ollamaã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã—ã¾ã—ãŸ")
                    return True
                time.sleep(1)
        return False
    return True

def check_model_exists(model_name):
    """ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        result = subprocess.run(['ollama', 'list'], 
                              capture_output=True, 
                              text=True,
                              timeout=5)
        return model_name.split(':')[0] in result.stdout
    except:
        return False

def pull_model_if_needed(model_name):
    """å¿…è¦ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ«"""
    try:
        if not check_model_exists(model_name):
            st.info(f"ãƒ¢ãƒ‡ãƒ« {model_name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            result = subprocess.run(['ollama', 'pull', model_name], 
                                  capture_output=True, 
                                  text=True,
                                  timeout=300)  # 5åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            if result.returncode == 0:
                st.success(f"ãƒ¢ãƒ‡ãƒ« {model_name} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                return True
            else:
                st.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {result.stderr}")
                return False
        return True
    except subprocess.TimeoutExpired:
        st.error("ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
        return False
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return False

# --- Streamlit UI ---
st.set_page_config(page_title="YouTube + Gemma3 ã‚³ãƒ©ãƒ æ¡ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿", layout="centered")
st.title("YouTube + Gemma3 ã‚³ãƒ©ãƒ æ¡ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿")

# --- ãƒ¢ãƒ‡ãƒ«åˆ‡æ›¿UIï¼ˆ4b / 12bï¼‰ ---
MODEL_OPTIONS = ("gemma3:latest", "gemma3:12b")

# æ—¢å®šã¯12bï¼ˆå¿…è¦ãªã‚‰4bã«å¤‰æ›´OKï¼‰
if "ollama_model" not in st.session_state:
    st.session_state["ollama_model"] = "gemma3:latest"

with st.sidebar:
    st.subheader("Ollama ãƒ¢ãƒ‡ãƒ«åˆ‡æ›¿")
    st.session_state["ollama_model"] = st.radio(
        "Gemma3 ã‚µã‚¤ã‚º",
        MODEL_OPTIONS,
        index=MODEL_OPTIONS.index(st.session_state["ollama_model"]),
        horizontal=True
    )
    st.caption(f"ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«: **{st.session_state['ollama_model']}**")

def current_model() -> str:
    """é¸æŠä¸­ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™ï¼ˆollama.chat ã§ä½¿ã†ï¼‰"""
    return st.session_state["ollama_model"]

CACHE_DIR = ""  # CSV ã‚’ä¿å­˜ã—ã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€

# --- YouTube API Key è¨­å®š ---
API_KEY = "AIzaSyC36r9O-Dx4-afYBS1Fpuf_P1K9wpVfsVo"
YOUTUBE_API_SERVICE_NAME = "youtube"
YOUTUBE_API_VERSION = "v3"

# --- ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã®OllamaåˆæœŸåŒ– ---
if 'ollama_initialized' not in st.session_state:
    with st.spinner("OllamaåˆæœŸåŒ–ä¸­..."):
        # Ollamaã‚µãƒ¼ãƒ“ã‚¹ã®ç¢ºèªã¨èµ·å‹•
        if ensure_ollama_running():
            # ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèªã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            if pull_model_if_needed(current_model()):
                st.session_state['ollama_initialized'] = True
                st.success("åˆæœŸåŒ–å®Œäº†")
            else:
                st.error("ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ")
                st.stop()
        else:
            st.error("Ollamaã‚µãƒ¼ãƒ“ã‚¹ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ")
            st.info("ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ 'ollama serve' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            st.stop()

# --- OllamaçŠ¶æ…‹è¡¨ç¤ºï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰ ---
st.sidebar.header("OllamaçŠ¶æ…‹")
ollama_status = check_ollama_status()
status_color = "ğŸŸ¢" if ollama_status else "ğŸ”´"
st.sidebar.write(f"{status_color} Ollama: {'ç¨¼åƒä¸­' if ollama_status else 'åœæ­¢ä¸­'}")

# æ‰‹å‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒœã‚¿ãƒ³
if st.sidebar.button("çŠ¶æ…‹ã‚’æ›´æ–°"):
    st.rerun()

# --- Functions ---
def fetch_youtube_data(query, max_results=200):
    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)
    video_data = []
    next_page_token = None

    def change_iso(duration_str):
        try:
            td = isodate.parse_duration(duration_str)
            return str(td)
        except Exception:
            return "0:00"

    while len(video_data) < max_results:
        remaining = max_results - len(video_data)
        search_response = youtube.search().list(
            q=query,
            part="id,snippet",
            maxResults=min(50, remaining),
            type="video",
            pageToken=next_page_token
        ).execute()

        video_ids = [item["id"]["videoId"] for item in search_response.get("items", [])]
        if not video_ids:
            break

        videos_response = youtube.videos().list(
            id=','.join(video_ids),
            part="snippet,statistics,contentDetails"
        ).execute()

        for item in videos_response.get("items", []):
            video_data.append({
                "videoId": item["id"],
                "title": item["snippet"]["title"],
                "viewCount": int(item["statistics"].get("viewCount", 0)),
                "likeCount": int(item["statistics"].get("likeCount", 0)),
                "duration": change_iso(item["contentDetails"]["duration"]),
                "description": item["snippet"].get("description", "")
            })

        next_page_token = search_response.get("nextPageToken")
        if not next_page_token:
            break

    return pd.DataFrame(video_data)

def parse_google_trends_csv(uploaded_file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸGoogle Trendsã®CSVã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹"""
    if uploaded_file is None:
        return None
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒ³ã‚¿ã‚’æœ€åˆã«æˆ»ã—ã¦ã‹ã‚‰èª­ã¿è¾¼ã‚€
        uploaded_file.seek(0)
        content = uploaded_file.getvalue().decode("utf-8")
        lines = content.split('\n')

        top_keywords = []
        rising_keywords = []
        current_section = None

        top_header = 'TOP'
        rising_header = 'RISING'
        
        data_started = False
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if top_header in line:
                current_section = 'top'
                data_started = True
                continue
            elif rising_header in line:
                current_section = 'rising'
                data_started = True
                continue
            
            if not data_started or "ã‚«ãƒ†ã‚´ãƒª:" in line:
                continue

            parts = line.split(',')
            if len(parts) >= 2:
                keyword = parts[0].strip('"')
                value = ','.join(parts[1:]).strip('"')
                if current_section == 'top':
                    top_keywords.append({'keyword': keyword, 'score': value})
                elif current_section == 'rising':
                    rising_keywords.append({'keyword': keyword, 'increase': value})

        df_top = pd.DataFrame(top_keywords)
        df_rising = pd.DataFrame(rising_keywords)
        
        if not df_top.empty:
            df_top.rename(columns={'score': 'importance'}, inplace=True)
        if not df_rising.empty:
            df_rising.rename(columns={'increase': 'importance'}, inplace=True)
        
        if not df_top.empty and not df_rising.empty:
            return pd.concat([df_top, df_rising]).dropna().reset_index(drop=True)
        elif not df_top.empty:
            return df_top
        elif not df_rising.empty:
            return df_rising
        else:
            return pd.DataFrame()

    except Exception as e:
        st.error(f"Googleãƒˆãƒ¬ãƒ³ãƒ‰CSVã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def generate_suggestions(df, query, trend_df=None, num=5, save_path=None):
    if save_path:
        save_file = save_path
    else:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_file = f"suggestions_{ts}.csv"

    # ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„ã‚’ä½œæˆï¼ˆæœ€åˆã®10ä»¶ã®ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ä½¿ç”¨ï¼‰
    sample_titles = df.head(10)['title'].tolist()
    titles_text = "\n".join(sample_titles)
    
    # Googleãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
    trend_text = ""
    if trend_df is not None and not trend_df.empty:
        trend_text += "\n\nã•ã‚‰ã«ã€é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦Googleãƒˆãƒ¬ãƒ³ãƒ‰ã§ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ãŒå¾—ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚\n"
        trend_text += "ã“ã‚Œã‚‰ã¯ç¾åœ¨æ³¨ç›®åº¦ãŒé«˜ã„ã€ã¾ãŸã¯æ€¥ä¸Šæ˜‡ã—ã¦ã„ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã™ã€‚\n\n"
        trend_text += trend_df.to_string(index=False)
    
    prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{query}ã€ã§è¦‹ã¤ã‹ã£ãŸå‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã®ä¾‹ï¼š

{titles_text}
{trend_text}

ã“ã‚Œã‚‰ã®å‹•ç”»ã®å…±é€šãƒ†ãƒ¼ãƒã‚„ã€Googleãƒˆãƒ¬ãƒ³ãƒ‰ã§æ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è€ƒæ…®ã—ã¦ã€æ–°ã—ã„ãƒˆãƒ”ãƒƒã‚¯ã‚’{num}å€‹ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ç‰¹ã«Googleãƒˆãƒ¬ãƒ³ãƒ‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯é‡è¦åº¦ãŒé«˜ã„ã®ã§ã€ç©æ¥µçš„ã«å«ã‚ã¦ãã ã•ã„ã€‚
åœ°åãƒ»è£½å“åãƒ»äººç‰©åãªã©ã®å›ºæœ‰åã¯é¿ã‘ã€ä¸€èˆ¬çš„ãªãƒˆãƒ”ãƒƒã‚¯ã«ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®CSVå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
```csv
keyword
ãƒˆãƒ”ãƒƒã‚¯1
ãƒˆãƒ”ãƒƒã‚¯2
...
"""

    try:
        response = ollama.chat(
            model=current_model(),
            messages=[{'role': 'user', 'content': prompt}],
            options={'temperature': 0.7, 'num_predict': 500}
        )
        text = response['message']['content']
    except Exception as e:
        st.error(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()

    # CSVéƒ¨åˆ†ã‚’æŠ½å‡º
    m = re.search(r"```csv\n(.*?)```", text, re.DOTALL)
    if m:
        csv_content = m.group(1).strip()
    else:
        # ```csv ãŒãªã„å ´åˆã€keyword ã§å§‹ã¾ã‚‹è¡Œã‚’æ¢ã™
        lines = text.split('\n')
        csv_lines = []
        in_csv = False
        for line in lines:
            if 'keyword' in line.lower():
                in_csv = True
            if in_csv and line.strip():
                csv_lines.append(line.strip())
        csv_content = '\n'.join(csv_lines)

    try:
        sug_df = pd.read_csv(io.StringIO(csv_content))
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ normalize
        sug_df.columns = [c.strip().lower().replace("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "keyword") for c in sug_df.columns]

        if 'keyword' not in sug_df.columns:
            st.error("CSVã« 'keyword' åˆ—ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return pd.DataFrame()

        sug_df = sug_df[['keyword']]
        sug_df.to_csv(save_file, index=False)
        st.success(f"ææ¡ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä¿å­˜: {save_file}")

        return sug_df

    except Exception as e:
        st.error(f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()


def compose_structure(base_kw, suggestion, target="ç‰¹ã«ãªã—", duration=15, purpose="ç‰¹ã«ãªã—", save_dir="outlines", sections=4):
    full_topic = f"{base_kw} {suggestion}"

    conditions = [f"- {sections}ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆè¦‹å‡ºã—ã¨æœ¬æ–‡ï¼‰ã«åˆ†ã‘ã‚‹"]
    estimated_chars = duration * 400 
    conditions.append(f"- å…¨ä½“ã§ç´„{estimated_chars}å­—ç¨‹åº¦ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ")
    if target and target != "ç‰¹ã«ãªã—":
        conditions.append(f"- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…: {target}")
    if purpose and purpose != "ç‰¹ã«ãªã—":
        conditions.append(f"- è¨˜äº‹ã®ç›®çš„: {purpose}")
    conditions += ["- å°‚é–€ç”¨èªã¯é¿ã‘ã€åˆå¿ƒè€…ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ã„æ–‡ç« "]
    
    prompt = f"""
ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸãƒ†ãƒ¼ãƒã¨æ¡ä»¶ã«åŸºã¥ãã€èª­è€…ã®çŸ¥çš„å¥½å¥‡å¿ƒã‚’æº€ãŸã™ã‚ˆã†ãªã€è³ªã®é«˜ã„ã‚³ãƒ©ãƒ è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

## ãƒ†ãƒ¼ãƒ: ã€Œ{full_topic}ã€

## æ¡ä»¶:
{chr(10).join(conditions)}

## å‚è€ƒã«ã™ã‚‹ã‚³ãƒ©ãƒ ã®å½¢å¼ä¾‹ï¼š
ã“ã‚Œã¯ã‚ãªãŸãŒç›®æŒ‡ã™ã¹ãæ–‡ç« ã®ã‚¹ã‚¿ã‚¤ãƒ«ã§ã™ã€‚å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€å˜ãªã‚‹æƒ…å ±ã®ç¾…åˆ—ã§ã¯ãªãã€èƒŒæ™¯ã‚„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æ„Ÿã˜ã•ã›ã‚‹è§£èª¬æ–‡ã«ã—ã¦ãã ã•ã„ã€‚

### ã‚¿ã‚¤ãƒˆãƒ«: ãƒ†ãƒ¼ãƒã®æ ¸å¿ƒã‚’ã¤ãã€èª­è€…ãŒã‚¯ãƒªãƒƒã‚¯ã—ãŸããªã‚‹ã‚ˆã†ãªã‚‚ã®ã€‚
#### ã‚«ãƒ†ã‚´ãƒªãƒ¼: è¨˜äº‹ã®å†…å®¹ã‚’çš„ç¢ºã«è¡¨ã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’3ã¤ã€‚
##### è¦‹å‡ºã—ã¨æœ¬æ–‡: å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¯ã€å†…å®¹ã‚’è¦ç´„ã—ãŸã€Œè¦‹å‡ºã—ã€ã¨ã€èƒŒæ™¯ã‚„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æ„Ÿã˜ã•ã›ã‚‹ç´„300å­—ç¨‹åº¦ã®ã€Œæœ¬æ–‡ã€ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚æœ¬æ–‡ã¯å˜ãªã‚‹æƒ…å ±ã®ç¾…åˆ—ã§ã¯ãªãã€èª­è€…ã«èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ãªã‚¹ã‚¿ã‚¤ãƒ«ã§è¨˜è¿°ã—ã¾ã™ã€‚

## å‡ºåŠ›å½¢å¼:
ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"body_text"ã¯ã€å‚è€ƒä¾‹ã®ï¼ˆå‚è€ƒæœ¬æ–‡ï¼‰ã®ã‚ˆã†ã«ã€ãã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§èªã‚‹ã¹ãå†…å®¹ã‚’èª­è€…ã«èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ãªã€200ã€œ300å­—ç¨‹åº¦ã®æœ¬æ–‡ã¨ã—ã¦è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

```json
{{
    "title": "è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«æ¡ˆ",
    "category": "è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼ˆç°¡æ½”ãªã‚‚ã®ã‚’3å€‹ç¨‹åº¦ï¼‰",
    "sections": [
        {{
            "heading": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³1ã®è¦‹å‡ºã—",
            "body_text": "ï¼ˆã“ã“ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³1ã®æœ¬æ–‡ã‚’200ã€œ300å­—ã§è¨˜è¿°ï¼‰"
        }},
        {{
            "heading": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³2ã®è¦‹å‡ºã—",
            "body_text": "ï¼ˆã“ã“ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³2ã®æœ¬æ–‡ã‚’200ã€œ300å­—ã§è¨˜è¿°ï¼‰"
        }}
    ]
}}
"""
    try:
        response = ollama.chat(
        model=current_model(),
        messages=[{'role': 'user', 'content': prompt}],
        options={'temperature': 0.7, 'num_predict': 4000} #æ–‡å­—æ•°ãŒå¢—ãˆã‚‹ãŸã‚å°‘ã—å¢—ã‚„ã™
        )
        response_text = response['message']['content']

        json_match = re.search(r"```json\n(.*?)\n```", response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            json_str = response_text[response_text.find('{'):response_text.rfind('}')+1]

        data = json.loads(json_str)

        # æ–°ã—ã„å½¢å¼ã§Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        md_output = f"# {data.get('title', 'No Title')}\n\n"
        md_output += f"ã‚«ãƒ†ã‚´ãƒªãƒ¼: {data.get('category', 'N/A')}\n\n"
        
        for section in data.get('sections', []):
            md_output += f"## {section.get('heading', 'No Heading')}\n\n"
            md_output += f"{section.get('body_text', '')}\n\n" 
        
        os.makedirs(save_dir, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_base = re.sub(r'[^\w\s-]', '', base_kw)[:20]
        safe_sug = re.sub(r'[^\w\s-]', '', suggestion)[:20]
        filename = f"{safe_base}_{safe_sug}_{ts}.md"
        path = os.path.join(save_dir, filename)
        
        with open(path, "w", encoding="utf-8") as f:
            f.write(md_output)

        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ–°ã—ã„å½¢å¼ã§ä¿å­˜
        json_filename = f"{safe_base}_{safe_sug}_{ts}.json"
        json_path = os.path.join(save_dir, json_filename)
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        
        st.success(f"ã‚³ãƒ©ãƒ ä¿å­˜å®Œäº†: {filename}")

        return data
    except json.JSONDecodeError:
        st.error(f"JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã€‚LLMã®å‡ºåŠ›å½¢å¼ãŒä¸æ­£ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        st.text_area("LLMã‹ã‚‰ã®ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹", response_text)
        return None
    except Exception as e:
        st.error(f"æ§‹æˆæ¡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- ãƒ¡ã‚¤ãƒ³UI ---
st.markdown("---")

# ã‚¹ãƒ†ãƒƒãƒ—1: æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰å…¥åŠ› & ãƒ‡ãƒ¼ã‚¿å–å¾—
st.subheader("ã‚¹ãƒ†ãƒƒãƒ—1: åˆ†æã®å…ƒã¨ãªã‚‹æƒ…å ±ã‚’å…¥åŠ›")
base_query = st.text_input("â‘  æ¤œç´¢ã®è»¸ã¨ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š", help="YouTubeã§æ¤œç´¢ã™ã‚‹éš›ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã™ã€‚")

# â–¼â–¼â–¼ å¤‰æ›´ç‚¹1: ãƒ©ãƒ™ãƒ«ã‹ã‚‰ã€Œ(ä»»æ„)ã€ã‚’å‰Šé™¤ â–¼â–¼â–¼
uploaded_trend_file = st.file_uploader(
    "â‘¡ Googleãƒˆãƒ¬ãƒ³ãƒ‰ã®é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", 
    type=['csv'],
    help="Googleãƒˆãƒ¬ãƒ³ãƒ‰ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã€Œé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¾ã™ã€‚"
)

# ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚‰ã™ãã«ãƒ‘ãƒ¼ã‚¹ã—ã¦session_stateã«ä¿å­˜
if uploaded_trend_file:
    trend_df = parse_google_trends_csv(uploaded_trend_file)
    if trend_df is not None:
        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸå¾Œã«è¡¨ç¤ºã—ãŸæ–¹ãŒUIãŒã™ã£ãã‚Šã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã‚‚è‰¯ã„
        # st.success("Googleãƒˆãƒ¬ãƒ³ãƒ‰CSVã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        st.session_state['trend_df'] = trend_df
else:
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠè§£é™¤ã•ã‚ŒãŸå ´åˆã«å‚™ãˆã¦ã€session_stateã‹ã‚‰ã‚­ãƒ¼ã‚’å‰Šé™¤
    if 'trend_df' in st.session_state:
        del st.session_state['trend_df']


if st.button("YouTube ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"):
    # â–¼â–¼â–¼ å¤‰æ›´ç‚¹2: å¿…é ˆãƒã‚§ãƒƒã‚¯å‡¦ç†ã‚’è¿½åŠ  â–¼â–¼â–¼
    # ã¾ãšAPIã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
    if not API_KEY or "YOUR_API_KEY" in API_KEY:
        st.error("YouTube API ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    # æ¬¡ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚’ãƒã‚§ãƒƒã‚¯
    elif not base_query or not uploaded_trend_file:
        st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å…¥åŠ›ã¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®ä¸¡æ–¹ãŒå¿…è¦ã§ã™ã€‚")
    # ã™ã¹ã¦OKã®å ´åˆã®ã¿ãƒ‡ãƒ¼ã‚¿å–å¾—å‡¦ç†ã«é€²ã‚€
    else:
        with st.spinner("YouTube ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
            pattern = os.path.join(CACHE_DIR, f"{base_query}_*.csv") if CACHE_DIR else f"{base_query}_*.csv"
            cached = glob.glob(pattern)

            if cached:
                latest = max(cached, key=os.path.getmtime)
                df = pd.read_csv(latest)
                st.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨: {os.path.basename(latest)}")
            else:
                df = fetch_youtube_data(base_query)
                if not df.empty:
                    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
                    fname = f"{base_query}_{ts}.csv"
                    save_path = os.path.join(CACHE_DIR, fname) if CACHE_DIR else fname
                    df.to_csv(save_path, index=False)
                    st.success(f"ä¿å­˜å®Œäº†: {fname}")

            if df.empty:
                st.error("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                st.dataframe(df.head(10))
                st.info(f"å…¨{len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
                st.session_state['youtube_df'] = df


# ã‚¹ãƒ†ãƒƒãƒ—2: ææ¡ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ
if 'youtube_df' in st.session_state:
    st.markdown("---")
    st.subheader("ã‚¹ãƒ†ãƒƒãƒ—2: ææ¡ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ")
    
    num_keywords = st.slider("ç”Ÿæˆã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", 5, 20, 10)
    if st.button("ææ¡ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"):
        df_in = st.session_state['youtube_df']
        trend_df_in = st.session_state.get('trend_df', None)
        with st.spinner("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆä¸­..."):
            sug_df = generate_suggestions(df_in, query=base_query, trend_df=trend_df_in, num=num_keywords)
            if not sug_df.empty:
                st.session_state['suggestions'] = sug_df
                st.dataframe(sug_df)

# ã‚¹ãƒ†ãƒƒãƒ—3: æ§‹æˆç”Ÿæˆ
if 'suggestions' in st.session_state:
    st.markdown("---")
    st.subheader("ã‚¹ãƒ†ãƒƒãƒ—3: ã‚³ãƒ©ãƒ æ§‹æˆæ¡ˆã‚’ç”Ÿæˆ")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        purpose = st.text_input("è¨˜äº‹ã®ç›®çš„", "")
    with col2:
        target = st.text_input("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…", "")
    with col3: 
        sections = st.number_input("ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°", 1, 10, 4)
    with col4:
        duration = st.number_input("è¨˜äº‹ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ (åˆ†)", 3, 20, step=1, help="æƒ³å®šã™ã‚‹èª­äº†æ™‚é–“ã‚’åˆ†ã§æŒ‡å®š")
    
    selected_keywords = st.multiselect(
        "ã‚³ãƒ©ãƒ ã‚’ç”Ÿæˆã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸æŠ:",
        st.session_state['suggestions']['keyword'].tolist()
    )
    
    if selected_keywords and st.button("ã‚³ãƒ©ãƒ æ§‹æˆæ¡ˆã‚’ç”Ÿæˆ"):
        for kw in selected_keywords:
            with st.spinner(f"ã€Œ{kw}ã€ã®ã‚³ãƒ©ãƒ ã‚’ç”Ÿæˆä¸­..."):
                article_data = compose_structure(
                    base_query, kw,
                    sections=sections,
                    duration=duration,
                    target=target or "ç‰¹ã«ãªã—",
                    purpose=purpose or "ç‰¹ã«ãªã—"
                )
                
                if article_data:
                    # expanderå†…ã«è¨˜äº‹å…¨ä½“ã‚’è¡¨ç¤º
                    expander_title = article_data.get('title', f'{base_query} + {kw}')
                    with st.expander(f"ğŸ“ **ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ©ãƒ ï¼š {expander_title}**", expanded=True):
                        
                        st.markdown(f"## {article_data.get('title', 'No Title')}")
                        st.markdown(f"**ã‚«ãƒ†ã‚´ãƒªãƒ¼:** {article_data.get('category', 'N/A')}")
                        st.markdown("---")
                        
                        for section in article_data.get('sections', []):
                            st.markdown(f"### {section.get('heading', 'No Heading')}")
                            st.write(section.get('body_text', ''))

# è£œè¶³æƒ…å ±
st.sidebar.markdown("---")
st.sidebar.info("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: " + current_model())
st.sidebar.info("ä¿å­˜å…ˆ: outlines/")
st.sidebar.caption("â€» OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")