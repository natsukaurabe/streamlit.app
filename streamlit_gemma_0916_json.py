import streamlit as st
import pandas as pd
import io
from datetime import datetime
import isodate
import re
from googleapiclient.discovery import build
import ollama
import subprocess
import time
import requests
import os
import glob
import json

# --- Ollamaã®ç¢ºèª ---
def check_ollama_status():
    """Ollamaã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹"""
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=2)
        return response.status_code == 200
    except:
        return False

def start_ollama_service():
    """Ollamaã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•ã™ã‚‹ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰"""
    try:
        # Windowsã®å ´åˆ
        if os.name == 'nt':
            subprocess.Popen(['ollama', 'serve'], 
                           creationflags=subprocess.CREATE_NEW_CONSOLE)
        # Mac/Linuxã®å ´åˆ
        else:
            subprocess.Popen(['ollama', 'serve'], 
                           stdout=subprocess.DEVNULL, 
                           stderr=subprocess.DEVNULL)
        time.sleep(3)  # ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•å¾…ã¡
        return True
    except Exception as e:
        st.error(f"Ollamaèµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def ensure_ollama_running():
    """OllamaãŒå®Ÿè¡Œä¸­ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦èµ·å‹•"""
    if not check_ollama_status():
        st.info("Ollamaã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•ã—ã¦ã„ã¾ã™...")
        if start_ollama_service():
            # æœ€å¤§10ç§’å¾…æ©Ÿ
            for i in range(10):
                if check_ollama_status():
                    st.success("Ollamaã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã—ã¾ã—ãŸ")
                    return True
                time.sleep(1)
        return False
    return True

def check_model_exists(model_name):
    """ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        result = subprocess.run(['ollama', 'list'], 
                              capture_output=True, 
                              text=True,
                              timeout=5)
        return model_name.split(':')[0] in result.stdout
    except:
        return False

def pull_model_if_needed(model_name):
    """å¿…è¦ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ«"""
    try:
        if not check_model_exists(model_name):
            st.info(f"ãƒ¢ãƒ‡ãƒ« {model_name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            result = subprocess.run(['ollama', 'pull', model_name], 
                                  capture_output=True, 
                                  text=True,
                                  timeout=300)  # 5åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            if result.returncode == 0:
                st.success(f"ãƒ¢ãƒ‡ãƒ« {model_name} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                return True
            else:
                st.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {result.stderr}")
                return False
        return True
    except subprocess.TimeoutExpired:
        st.error("ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
        return False
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return False

# --- Streamlit UI ---
st.set_page_config(page_title="YouTube + Gemma3 å‹•ç”»æ§‹æˆæ¡ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿", layout="centered")
st.title("YouTube + Gemma3 å‹•ç”»æ§‹æˆæ¡ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿")

# --- ãƒ¢ãƒ‡ãƒ«åˆ‡æ›¿UIï¼ˆ4b / 12bï¼‰ ---
MODEL_OPTIONS = ("gemma3:4b", "gemma3:12b")

# æ—¢å®šã¯12bï¼ˆå¿…è¦ãªã‚‰4bã«å¤‰æ›´OKï¼‰
if "ollama_model" not in st.session_state:
    st.session_state["ollama_model"] = "gemma3:4b"

with st.sidebar:
    st.subheader("Ollama ãƒ¢ãƒ‡ãƒ«åˆ‡æ›¿")
    st.session_state["ollama_model"] = st.radio(
        "Gemma3 ã‚µã‚¤ã‚º",
        MODEL_OPTIONS,
        index=MODEL_OPTIONS.index(st.session_state["ollama_model"]),
        horizontal=True
    )
    st.caption(f"ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«: **{st.session_state['ollama_model']}**")

def current_model() -> str:
    """é¸æŠä¸­ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™ï¼ˆollama.chat ã§ä½¿ã†ï¼‰"""
    return st.session_state["ollama_model"]

CACHE_DIR = ""  # CSV ã‚’ä¿å­˜ã—ã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€

# --- YouTube API Key è¨­å®š ---
API_KEY = "AIzaSyC36r9O-Dx4-afYBS1Fpuf_P1K9wpVfsVo"
YOUTUBE_API_SERVICE_NAME = "youtube"
YOUTUBE_API_VERSION = "v3"

# --- ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã®OllamaåˆæœŸåŒ– ---
if 'ollama_initialized' not in st.session_state:
    with st.spinner("OllamaåˆæœŸåŒ–ä¸­..."):
        # Ollamaã‚µãƒ¼ãƒ“ã‚¹ã®ç¢ºèªã¨èµ·å‹•
        if ensure_ollama_running():
            # ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèªã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            if pull_model_if_needed(current_model()):
                st.session_state['ollama_initialized'] = True
                st.success("åˆæœŸåŒ–å®Œäº†")
            else:
                st.error("ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ")
                st.stop()
        else:
            st.error("Ollamaã‚µãƒ¼ãƒ“ã‚¹ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ")
            st.info("ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ 'ollama serve' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            st.stop()

# --- OllamaçŠ¶æ…‹è¡¨ç¤ºï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰ ---
st.sidebar.header("OllamaçŠ¶æ…‹")
ollama_status = check_ollama_status()
status_color = "ğŸŸ¢" if ollama_status else "ğŸ”´"
st.sidebar.write(f"{status_color} Ollama: {'ç¨¼åƒä¸­' if ollama_status else 'åœæ­¢ä¸­'}")

# æ‰‹å‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒœã‚¿ãƒ³
if st.sidebar.button("çŠ¶æ…‹ã‚’æ›´æ–°"):
    st.rerun()

# --- Functions ---
def fetch_youtube_data(query, max_results=200):
    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)
    video_data = []
    next_page_token = None

    def change_iso(duration_str):
        try:
            td = isodate.parse_duration(duration_str)
            return str(td)
        except Exception:
            return "0:00"

    while len(video_data) < max_results:
        remaining = max_results - len(video_data)
        search_response = youtube.search().list(
            q=query,
            part="id,snippet",
            maxResults=min(50, remaining),
            type="video",
            pageToken=next_page_token
        ).execute()

        video_ids = [item["id"]["videoId"] for item in search_response.get("items", [])]
        if not video_ids:
            break

        videos_response = youtube.videos().list(
            id=','.join(video_ids),
            part="snippet,statistics,contentDetails"
        ).execute()

        for item in videos_response.get("items", []):
            video_data.append({
                "videoId": item["id"],
                "title": item["snippet"]["title"],
                "viewCount": int(item["statistics"].get("viewCount", 0)),
                "likeCount": int(item["statistics"].get("likeCount", 0)),
                "duration": change_iso(item["contentDetails"]["duration"]),
                "description": item["snippet"].get("description", "")
            })

        next_page_token = search_response.get("nextPageToken")
        if not next_page_token:
            break

    return pd.DataFrame(video_data)

def generate_suggestions(df, query, num=5, save_path=None):
    if save_path:
        save_file = save_path
    else:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_file = f"suggestions_{ts}.csv"

    # ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„ã‚’ä½œæˆï¼ˆæœ€åˆã®10ä»¶ã®ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ä½¿ç”¨ï¼‰
    sample_titles = df.head(10)['title'].tolist()
    titles_text = "\n".join(sample_titles)
    
    prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{query}ã€ã§è¦‹ã¤ã‹ã£ãŸå‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã®ä¾‹ï¼š

{titles_text}

ã“ã‚Œã‚‰ã®å‹•ç”»ã®å…±é€šãƒ†ãƒ¼ãƒã‹ã‚‰ã€é–¢é€£ã™ã‚‹æ–°ã—ã„ãƒˆãƒ”ãƒƒã‚¯ã‚’{num}å€‹ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
åœ°åãƒ»è£½å“åãƒ»äººç‰©åãªã©ã®å›ºæœ‰åã¯é¿ã‘ã€ä¸€èˆ¬çš„ãªãƒˆãƒ”ãƒƒã‚¯ã«ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®CSVå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
```csv
keyword
ãƒˆãƒ”ãƒƒã‚¯1
ãƒˆãƒ”ãƒƒã‚¯2
...
```
"""

    try:
        response = ollama.chat(
            model=current_model(),
            messages=[{'role': 'user', 'content': prompt}],
            options={'temperature': 0.7, 'num_predict': 500}
        )
        text = response['message']['content']
    except Exception as e:
        st.error(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()
    
    # CSVéƒ¨åˆ†ã‚’æŠ½å‡º
    m = re.search(r"```csv\n(.*?)```", text, re.DOTALL)
    if m:
        csv_content = m.group(1).strip()
    else:
        # ```csv ãŒãªã„å ´åˆã€keyword ã§å§‹ã¾ã‚‹è¡Œã‚’æ¢ã™
        lines = text.split('\n')
        csv_lines = []
        in_csv = False
        for line in lines:
            if 'keyword' in line.lower():
                in_csv = True
            if in_csv and line.strip():
                csv_lines.append(line.strip())
        csv_content = '\n'.join(csv_lines)

    try:
        sug_df = pd.read_csv(io.StringIO(csv_content))
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ normalize
        sug_df.columns = [c.strip().lower().replace("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "keyword") for c in sug_df.columns]

        if 'keyword' not in sug_df.columns:
            st.error("CSVã« 'keyword' åˆ—ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return pd.DataFrame()

        sug_df = sug_df[['keyword']]
        sug_df.to_csv(save_file, index=False)
        st.success(f"ææ¡ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä¿å­˜: {save_file}")

        return sug_df

    except Exception as e:
        st.error(f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return pd.DataFrame()

def compose_structure(base_kw, suggestion, target="ç‰¹ã«ãªã—", duration=15, purpose="ç‰¹ã«ãªã—", save_dir="outlines", sections=4):
    full_topic = f"{base_kw} {suggestion}"

    conditions = [f"- {sections}ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†ã‘ã‚‹"]
    conditions.append(f"- å‹•ç”»ã®é•·ã•: {duration}åˆ†")
    if target and target != "ç‰¹ã«ãªã—":
        conditions.append(f"- å¯¾è±¡è¦–è´è€…: {target}")
    if purpose and purpose != "ç‰¹ã«ãªã—":
        conditions.append(f"- å‹•ç”»ã®ç›®çš„: {purpose}")
    conditions += ["- åˆå¿ƒè€…ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ã„æ§‹æˆ"]

    prompt = f"""
        YouTubeã®è§£èª¬ã®æ¦‚è¦æ–‡ã¨ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼Œã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã«å…¥ã‚Œã‚‹æ–‡è¨€ï¼Œå‹•ç”»ã®ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

        ãƒ†ãƒ¼ãƒ: ã€Œ{full_topic}ã€

        æ¡ä»¶:
        {chr(10).join(conditions)}

        ä»¥ä¸‹ã®ã‚­ãƒ¼ã‚’æŒã¤JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        - "title": "å‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«æ¡ˆ"
        - "summary": "å‹•ç”»ã®æ¦‚è¦æ–‡ï¼ˆèª¬æ˜æ¬„ç”¨ï¼‰"
        - "hashtags": ["ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°1", "ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°2", ...]
        - "keywords": ["é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2", ...]
        - "thumbnail_text": "ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã«å…¥ã‚Œã‚‹ã¨ã‚¯ãƒªãƒƒã‚¯ã•ã‚Œã‚„ã™ã„æ–‡è¨€"
        - "outline": [
            {{
                "section_title": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³1ã®ã‚¿ã‚¤ãƒˆãƒ«+æ™‚é–“(0:00~0:00)",
                "points": ["ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¦ç‚¹1", "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¦ç‚¹2", ...]
            }},
            {{
                "section_title": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³2ã®ã‚¿ã‚¤ãƒˆãƒ«+æ™‚é–“(0:00~0:00)",
                "points": ["ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¦ç‚¹1", "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¦ç‚¹2", ...]
            }}
        ]

        ```json
        {{
        // ã“ã“ã«JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆ
        }}
        ````
    """

    # ---------------------------------------------------------

    try:
        response = ollama.chat(
            model=current_model(),
            messages=[{'role': 'user', 'content': prompt}],
            options={'temperature': 0.7, 'num_predict': 2000}
        )
        response_text = response['message']['content']
        
        json_match = re.search(r"```json\n(.*?)\n```", response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            json_str = response_text[response_text.find('{'):response_text.rfind('}')+1]

        data = json.loads(json_str)

        # å¾“æ¥é€šã‚Šã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚‚è¡Œã†
        md_output = f"# {data.get('title', 'No Title')}\n\n"
        md_output += f"## æ¦‚è¦\n{data.get('summary', '')}\n\n"
        md_output += f"## ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°\n{' '.join(['#' + tag for tag in data.get('hashtags', [])])}\n\n"
        md_output += f"## ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\n{', '.join(data.get('keywords', []))}\n\n"
        md_output += f"## ã‚µãƒ ãƒã‚¤ãƒ«æ–‡è¨€\n> {data.get('thumbnail_text', '')}\n\n"
        md_output += "## å‹•ç”»æ§‹æˆæ¡ˆ\n"
        for i, section in enumerate(data.get('outline', [])):
            md_output += f"### {i+1}. {section.get('section_title', '')}\n"
            for point in section.get('points', []):
                md_output += f"- {point}\n"
            md_output += "\n"

        os.makedirs(save_dir, exist_ok=True)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_base = re.sub(r'[^\w\s-]', '', base_kw)[:20]
        safe_sug = re.sub(r'[^\w\s-]', '', suggestion)[:20]
        filename = f"{safe_base}_{safe_sug}_{ts}.md"
        path = os.path.join(save_dir, filename)
        
        with open(path, "w", encoding="utf-8") as f:
            f.write(md_output)
        st.success(f"ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ä¿å­˜: {path}")

        return data

    except json.JSONDecodeError:
        st.error(f"JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã€‚LLMã®å‡ºåŠ›å½¢å¼ãŒä¸æ­£ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        st.text_area("LLMã‹ã‚‰ã®ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹", response_text)
        return None
    except Exception as e:
        st.error(f"æ§‹æˆæ¡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- ãƒ¡ã‚¤ãƒ³UI ---
st.markdown("---")

# ã‚¹ãƒ†ãƒƒãƒ—1: æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰å…¥åŠ› & ãƒ‡ãƒ¼ã‚¿å–å¾—
base_query = st.text_input("æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š")
if st.button("YouTube ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"):
    if not API_KEY or API_KEY == "YOUR_API_KEY_HERE":
        st.error("YouTube API ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    else:
        with st.spinner("YouTube ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
            pattern = os.path.join(CACHE_DIR, f"{base_query}_*.csv") if CACHE_DIR else f"{base_query}_*.csv"
            cached = glob.glob(pattern)

            if cached:
                latest = max(cached, key=os.path.getmtime)
                df = pd.read_csv(latest)
                st.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨: {os.path.basename(latest)}")
            else:
                df = fetch_youtube_data(base_query)
                if not df.empty:
                    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
                    fname = f"{base_query}_{ts}.csv"
                    save_path = os.path.join(CACHE_DIR, fname) if CACHE_DIR else fname
                    df.to_csv(save_path, index=False)
                    st.success(f"ä¿å­˜å®Œäº†: {fname}")
                    # 

            if df.empty:
                st.error("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                st.dataframe(df.head(10))  # æœ€åˆã®10ä»¶ã‚’è¡¨ç¤º
                st.info(f"å…¨{len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
                st.session_state['youtube_df'] = df

# ã‚¹ãƒ†ãƒƒãƒ—2: ææ¡ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ
if 'youtube_df' in st.session_state:
    st.markdown("---")
    st.subheader("ã‚¹ãƒ†ãƒƒãƒ—2: ææ¡ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ")
    
    num_keywords = st.slider("ç”Ÿæˆã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", 5, 20, 10)
    if st.button("ææ¡ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"):
        df_in = st.session_state['youtube_df']
        with st.spinner("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆä¸­..."):
            sug_df = generate_suggestions(df_in, query=base_query, num=num_keywords)
            if not sug_df.empty:
                st.session_state['suggestions'] = sug_df
                st.dataframe(sug_df)

# ã‚¹ãƒ†ãƒƒãƒ—3: æ§‹æˆç”Ÿæˆ
if 'suggestions' in st.session_state:
    st.markdown("---")
    st.subheader("ã‚¹ãƒ†ãƒƒãƒ—3: å‹•ç”»æ§‹æˆæ¡ˆã‚’ç”Ÿæˆ")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        purpose = st.text_input("å‹•ç”»ã®ç›®çš„", "")
    with col2:
        target = st.text_input("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¦–è´è€…", "")
    
    selected_keywords = st.multiselect(
        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸æŠ:",
        st.session_state['suggestions']['keyword'].tolist()
    )
    with col3: 
        sections = st.number_input("ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°", 1, 10, 4)
    
    with col4:
        duration = st.number_input("å‹•ç”»ã®é•·ã•", 5, 30, step=5)
    
    if selected_keywords and st.button("æ§‹æˆæ¡ˆã‚’ç”Ÿæˆ"):
        for kw in selected_keywords:
            with st.spinner(f"ã€Œ{kw}ã€ã®æ§‹æˆæ¡ˆã‚’ç”Ÿæˆä¸­..."):
                # æˆ»ã‚Šå€¤ãŒè¾æ›¸ã«ãªã‚‹
                structured_outline = compose_structure(
                    base_query, kw,
                    sections=sections,
                    duration=duration,
                    target=target or "ç‰¹ã«ãªã—",
                    purpose=purpose or "ç‰¹ã«ãªã—"
                )
                
                # --- ã“ã“ã‹ã‚‰ãŒæ–°ã—ã„è¡¨ç¤ºéƒ¨åˆ† ---
                if structured_outline:
                    # expanderã®ã‚¿ã‚¤ãƒˆãƒ«ã«LLMãŒç”Ÿæˆã—ãŸã‚¿ã‚¤ãƒˆãƒ«ã‚’ä½¿ç”¨
                    expander_title = structured_outline.get('title', f'{base_query} + {kw}')
                    with st.expander(f"ğŸ“ **{expander_title}**", expanded=True):
                        
                        # st.tabsã§è¦‹ã‚„ã™ãæƒ…å ±ã‚’åˆ†é¡
                        tab1, tab2, tab3 = st.tabs(["**æ¦‚è¦**", "**å‹•ç”»æ§‹æˆæ¡ˆ**", "**ãƒ¡ã‚¿æƒ…å ±**"])

                        with tab1:
                            st.subheader("ğŸ–¼ï¸ ã‚µãƒ ãƒã‚¤ãƒ«æ¡ˆ")
                            # st.infoã§ã‚µãƒ ãƒã‚¤ãƒ«æ–‡è¨€ã‚’ç›®ç«‹ãŸã›ã‚‹
                            st.info(f"**{structured_outline.get('thumbnail_text', 'N/A')}**")
                            
                            st.subheader("ğŸ“„ æ¦‚è¦æ–‡")
                            st.write(structured_outline.get('summary', ''))

                        with tab2:
                            st.subheader("ğŸ¬ å‹•ç”»æ§‹æˆæ¡ˆ")
                            outline_sections = structured_outline.get('outline', [])
                            
                            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°ã«å¿œã˜ã¦ã‚«ãƒ©ãƒ æ•°ã‚’å‹•çš„ã«å¤‰æ›´ï¼ˆæœ€å¤§3ã‚«ãƒ©ãƒ ï¼‰
                            num_cols = min(len(outline_sections), 1) 
                            if num_cols > 0:
                                cols = st.columns(num_cols)
                                for i, section in enumerate(outline_sections):
                                    with cols[i % num_cols]:
                                        # st.container(border=True)ã§å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚«ãƒ¼ãƒ‰é¢¨ã«
                                        with st.container(border=True):
                                            st.markdown(f"**{i+1}. {section.get('section_title', 'No Title')}**")
                                            for point in section.get('points', []):
                                                st.markdown(f"- {point}")
                        
                        with tab3:
                            st.subheader("ğŸ’¡ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°")

                            # st.columnsã§æƒ…å ±ã‚’ä¸¦åˆ—ã«è¡¨ç¤º
                            col1, col2 = st.columns(2)
                            with col1:
                                 # st.metricã§å‹•ç”»ã®é•·ã•ã‚’å¼·èª¿
                                st.metric(label="å‹•ç”»ã®é•·ã•", value=f"{duration} åˆ†")

                            # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’ãƒãƒƒã‚¸é¢¨ã«è¡¨ç¤º
                            hashtags = structured_outline.get('hashtags', [])
                            if hashtags:
                                st.markdown("**ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°:** " + " ".join([f"`#{tag}`" for tag in hashtags]))
                            
                            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚åŒæ§˜ã«è¡¨ç¤º
                            keywords = structured_outline.get('keywords', [])
                            if keywords:
                                 st.markdown("**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:** " + " ".join([f"`{kw}`" for kw in keywords]))

# è£œè¶³æƒ…å ±
st.sidebar.markdown("---")
st.sidebar.info("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: " + current_model())
st.sidebar.info("ä¿å­˜å…ˆ: outlines/")
st.sidebar.caption("â€» OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")